{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute `huggingface-cli delete-cache` in the terminal to select which models you want to clear from the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import helpers\n",
    "from helpers import Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the papers\n",
    "\n",
    "with open('papers.pkl', 'rb') as f:\n",
    "    papers: list[Paper] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panag\\Desktop\\captonomy-mle\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "model_name = 'facebook/opt-125m' # 251.9MB\n",
    "# model_name = 'gpt2' # 551.0MB\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Here is the title and abstract of a scientific paper:\n",
    "Title: {paper_title}\n",
    "Abstract: {paper_abstract}\n",
    "Please classify the paper into one of the following categories:\n",
    "0. Tables\n",
    "1. Classification\n",
    "2. Key Information Extraction\n",
    "3. Optical Character Recognition\n",
    "4. Datasets\n",
    "5. Document Layout Understanding\n",
    "If the paper does not fit into any of the above categories, please select '6'.\n",
    "The number of the correct category for this paper is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [02:51<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "output_texts = []\n",
    "\n",
    "for paper in tqdm(papers):    \n",
    "    input_text = prompt_template.format(paper_title=paper.title, paper_abstract=paper.abstract)\n",
    "    input = tokenizer(input_text, return_tensors='pt')\n",
    "    output = model.generate(**input, max_new_tokens=1)\n",
    "    output_text = tokenizer.decode(output[0][-1], skip_special_tokens=True)\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "predictions = list(map(lambda index: helpers.categories[int(index)], output_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "\n",
    "helpers.save_to_csv(papers, predictions, 'generation-clm.csv')\n",
    "\n",
    "helpers.save_to_json(papers, predictions, 'generation-clm.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction-tuned Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" # 3.1GB (long inference time)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"Please classify the given paper into one of the following categories:\n",
    "0. Tables\n",
    "1. Classification\n",
    "2. Key Information Extraction\n",
    "3. Optical Character Recognition\n",
    "4. Datasets\n",
    "5. Document Layout Understanding\n",
    "If the paper does not fit into any of the above categories, please select '6'.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"Here is the title and abstract of a scientific paper:\n",
    "Title: {paper_title}\n",
    "Abstract: {paper_abstract}\n",
    "The number of the correct category for this paper is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [31:59<00:00, 12.97s/it]\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "responses = []\n",
    "\n",
    "for i in tqdm(range(len(papers))):\n",
    "    \n",
    "    input_text = prompt_template.format(paper_title=paper.title, paper_abstract=paper.abstract)\n",
    "\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": input_text}\n",
    "    ]\n",
    "\n",
    "    messages.append(message)\n",
    "\n",
    "    if len(messages) == 8 or i == len(papers) - 1:\n",
    "\n",
    "        texts = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        model_inputs = tokenizer(texts, return_tensors=\"pt\")\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=1)\n",
    "        response = tokenizer.batch_decode(generated_ids[:,-1], skip_special_tokens=True)\n",
    "        responses.extend(response)\n",
    "\n",
    "        messages = []\n",
    "\n",
    "predictions = list(map(lambda index: helpers.categories[int(index)], responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "\n",
    "helpers.save_to_csv(papers, predictions, 'generation-itlm.csv')\n",
    "\n",
    "helpers.save_to_json(papers, predictions, 'generation-itlm.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
