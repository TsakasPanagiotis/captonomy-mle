{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute `huggingface-cli delete-cache` in the terminal to select which models you want to clear from the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Paper:\n",
    "    filename: str\n",
    "    title: str = ''\n",
    "    authors: str = ''\n",
    "    abstract: str = ''\n",
    "    keywords: str = ''\n",
    "    introduction: str = ''\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f' filename \\n----------\\n {self.filename}' + \\\n",
    "               f'\\n\\n title \\n----------\\n {self.title}' + \\\n",
    "               f'\\n\\n authors \\n----------\\n {self.authors}' + \\\n",
    "               f'\\n\\n abstract \\n----------\\n {self.abstract}' + \\\n",
    "               f'\\n\\n keywords \\n----------\\n {self.keywords}' + \\\n",
    "               f'\\n\\n introduction \\n----------\\n {self.introduction}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the papers and encode them into embeddings\n",
    "\n",
    "with open('papers.pkl', 'rb') as f:\n",
    "    papers: list[Paper] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panag\\Desktop\\captonomy-mle\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "model_name = 'facebook/opt-125m' # 251.9MB\n",
    "# model_name = 'gpt2' # 551.0MB\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Here is the title and abstract of a scientific paper:\n",
    "Title: {paper_title}\n",
    "Abstract: {paper_abstract}\n",
    "Please classify the paper into one of the following categories:\n",
    "0. Tables\n",
    "1. Classification\n",
    "2. Key Information Extraction\n",
    "3. Optical Character Recognition\n",
    "4. Datasets\n",
    "5. Document Layout Understanding\n",
    "If the paper does not fit into any of the above categories, please select '6'.\n",
    "The number of the correct category for this paper is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [03:18<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_texts = []\n",
    "\n",
    "for paper in tqdm(papers):    \n",
    "    input_text = prompt_template.format(paper_title=paper.title, paper_abstract=paper.abstract)\n",
    "    input = tokenizer(input_text, return_tensors='pt')\n",
    "    output = model.generate(**input, max_new_tokens=1)\n",
    "    output_text = tokenizer.decode(output[0][-1], skip_special_tokens=True)\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "print(output_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction-tuned Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" # 3.1GB\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"Please classify the given paper into one of the following categories:\n",
    "0. Tables\n",
    "1. Classification\n",
    "2. Key Information Extraction\n",
    "3. Optical Character Recognition\n",
    "4. Datasets\n",
    "5. Document Layout Understanding\n",
    "If the paper does not fit into any of the above categories, please select '6'.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"Here is the title and abstract of a scientific paper:\n",
    "Title: {paper_title}\n",
    "Abstract: {paper_abstract}\n",
    "The number of the correct category for this paper is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [29:44<00:00, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '5', '5', '5', '5', '5', '5', '5', '5', '5', '1', '5', '5', '5', '1', '5', '5', '5', '1', '5', '5', '5', '5', '5', '5', '5', '5', '5', '1', '5', '1', '5', '5', '5', '1', '5', '5', '5', '5', '1', '5', '5', '1', '5', '5', '1', '5', '5', '5', '5', '5', '5', '5', '5', '1', '5', '1', '1', '1', '5', '1', '5', '5', '5', '5', '1', '5', '5', '1', '5', '1', '1', '5', '5', '5', '5', '5', '5', '1', '1', '1', '5', '5', '1', '1', '5', '5', '5', '5', '5', '5', '5', '5', '1', '5', '5', '5', '5', '5', '1', '5', '1', '5', '5', '5', '1', '1', '5', '1', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '1', '1', '1', '5', '5', '1', '5', '5', '5', '1', '5', '5', '1', '5', '5', '5', '5', '5', '1', '1', '5', '5', '5', '5', '5', '5', '5', '5', '5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "responses = []\n",
    "\n",
    "for i in tqdm(range(len(papers))):\n",
    "    \n",
    "    input_text = prompt_template.format(paper_title=paper.title, paper_abstract=paper.abstract)\n",
    "\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": input_text}\n",
    "    ]\n",
    "\n",
    "    messages.append(message)\n",
    "\n",
    "    if len(messages) == 8 or i == len(papers) - 1:\n",
    "\n",
    "        texts = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        model_inputs = tokenizer(texts, return_tensors=\"pt\")\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=1)\n",
    "\n",
    "        response = tokenizer.batch_decode(generated_ids[:,-1], skip_special_tokens=True)\n",
    "\n",
    "        responses.extend(response)\n",
    "\n",
    "        messages = []\n",
    "\n",
    "print(responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
