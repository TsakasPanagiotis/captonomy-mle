{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import pickle\n",
    "import zipfile\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pypdf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Paper:\n",
    "    filename: str\n",
    "    title: str = ''\n",
    "    authors: str = ''\n",
    "    abstract: str = ''\n",
    "    keywords: str = ''\n",
    "    introduction: str = ''    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f' filename \\n----------\\n {self.filename}' + \\\n",
    "               f'\\n\\n title \\n----------\\n {self.title}' + \\\n",
    "               f'\\n\\n authors \\n----------\\n {self.authors}' + \\\n",
    "               f'\\n\\n abstract \\n----------\\n {self.abstract}' + \\\n",
    "               f'\\n\\n keywords \\n----------\\n {self.keywords}' + \\\n",
    "               f'\\n\\n introduction \\n----------\\n {self.introduction}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    cleaned_text = url_pattern.sub('', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    text = re.sub(r'\\n', ' ', text)  # newlines\n",
    "    text = re.sub(r'\\s+', ' ', text) # extra spaces\n",
    "    text = text.strip()              # leading and trailing spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 103/149 [00:10<00:04, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difficulty with layout in ICDAR2024_proceedings_pdfs/0191.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:14<00:00, 10.44it/s]\n"
     ]
    }
   ],
   "source": [
    "zip_path = 'ml-engineer/ICDAR2024_papers.zip'\n",
    "\n",
    "papers: list[Paper] = []\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    for file_info in tqdm(zip_ref.infolist()):\n",
    "        if not file_info.is_dir() and file_info.filename.lower().endswith('.pdf'):\n",
    "            \n",
    "            paper = Paper(filename=file_info.filename)\n",
    "            papers.append(paper)\n",
    "            \n",
    "            with zip_ref.open(paper.filename) as pdf_file:\n",
    "                pdf_reader = pypdf.PdfReader(io.BytesIO(pdf_file.read()))\n",
    "                first_page = pdf_reader.pages[0]          \n",
    "                \n",
    "                ## PLAIN EXTRACTION ##\n",
    "\n",
    "                text = first_page.extract_text(extraction_mode=\"plain\")\n",
    "                \n",
    "                # Split abstract only once\n",
    "                abstract_split = re.split(r'Abstract\\.?', text, maxsplit=1)\n",
    "\n",
    "                if len(abstract_split) == 1:\n",
    "                    print(f'No abstract found in {paper.filename}')\n",
    "                    continue\n",
    "                \n",
    "                header = abstract_split[0]\n",
    "                abstract = abstract_split[1]\n",
    "                text = abstract\n",
    "                \n",
    "                # Split keywords only once\n",
    "                keywords_split = re.split(r'Keywords:?', text, maxsplit=1)\n",
    "                \n",
    "                # if len(keywords_split) == 1: pass # No keywords found\n",
    "\n",
    "                if len(keywords_split) == 2:\n",
    "                    abstract = keywords_split[0]\n",
    "                    keywords = keywords_split[1]\n",
    "                    text = keywords                \n",
    "                \n",
    "                # Split at introduction only once\n",
    "                introduction_split = re.split(r'1\\s*Introduction|Introduction', text, maxsplit=1)\n",
    "                \n",
    "                # if len(introduction_split) == 1: pass # No introduction found\n",
    "\n",
    "                if len(introduction_split) == 2:\n",
    "                    if len(keywords_split) == 1:\n",
    "                        abstract = introduction_split[0]\n",
    "                        introduction = introduction_split[1]\n",
    "                    if len(keywords_split) == 2:\n",
    "                        keywords = introduction_split[0]\n",
    "                        introduction = introduction_split[1]\n",
    "\n",
    "                abstract = remove_urls(abstract)\n",
    "                abstract = remove_whitespace(abstract)\n",
    "                paper.abstract = abstract\n",
    "\n",
    "                keywords = remove_urls(keywords)\n",
    "                keywords = remove_whitespace(keywords)\n",
    "                paper.keywords = keywords\n",
    "\n",
    "                introduction = remove_urls(introduction)\n",
    "                introduction = remove_whitespace(introduction)\n",
    "                paper.introduction = introduction\n",
    "                \n",
    "                ## LAYOUT EXTRACTION ##\n",
    "\n",
    "                try:\n",
    "                    text = first_page.extract_text(extraction_mode=\"layout\")\n",
    "                except:\n",
    "                    print(f'Difficulty with layout in {paper.filename}')\n",
    "                    continue\n",
    "                \n",
    "                # Split abstract only once\n",
    "                abstract_split = re.split(r'Abstract\\.?', text, maxsplit=1)\n",
    "                \n",
    "                # Split title and authors\n",
    "                header_split = re.split(r'\\n\\n+', abstract_split[0].strip())\n",
    "\n",
    "                if len(header_split) == 1:\n",
    "                    print(f'Difficulty splitting header in {paper.filename}')\n",
    "                    continue\n",
    "\n",
    "                title, authors = header_split[0], header_split[1]\n",
    "                \n",
    "                title = remove_whitespace(title)\n",
    "                paper.title = title\n",
    "\n",
    "                authors = remove_whitespace(authors)\n",
    "                authors = re.sub(r'[^A-Za-z\\s,]', '', authors)          # remove non-alphabetical characters except spaces and commas\n",
    "                authors = re.sub(r'\\s+,', ',', authors)                 # remove whitespaces before commas\n",
    "                authors = re.sub(r',+', ',', authors)                   # replace multiple commas with one comma\n",
    "                authors = re.sub(r',(?!\\s)', ', ', authors)             # add space after comma if not present\n",
    "                authors = re.sub(r' and(?!\\s)', ' and ', authors)       # add space after 'and' if not present\n",
    "                authors = authors.strip()\n",
    "                authors = re.sub(r'\\s\\s+.*?,', '', authors)             # remove more than two whitespaces and anything after until the closest comma\n",
    "                authors = re.sub(r'\\s\\s+and', ' and', authors)          # replace more than two whitspaces before 'and' with one space\n",
    "                authors = re.sub(r'\\s\\s+.*?$', '', authors)             # remove trailing withespaces and anything after until the end\n",
    "                authors = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', authors)  # split small and capital letters\n",
    "                authors = re.sub(r',$', '', authors)                    # remove trailing comma\n",
    "                authors = re.sub(r', [a-zA-Z]$', '', authors)           # remove trailing comma and single letter\n",
    "                authors = re.sub(r', and ', ' and ', authors)           # remove comma before 'and'\n",
    "                paper.authors = authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save papers to a pickle file\n",
    "with open('papers.pkl', 'wb') as f:\n",
    "    pickle.dump(papers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " filename \n",
       "----------\n",
       " ICDAR2024_proceedings_pdfs/0004.pdf\n",
       "\n",
       " title \n",
       "----------\n",
       " SAGHOG: Self-Supervised Autoencoder for Generating HOG Features for Writer Retrieval\n",
       "\n",
       " authors \n",
       "----------\n",
       " Marco Peer, Florian Kleber and Robert Sablatnig\n",
       "\n",
       " abstract \n",
       "----------\n",
       " This paper introduces Saghog , a self-supervised pretraining strategy for writer retrieval using HOG features of the binarized input image. Our preprocessing involves the application of the Segment Any- thing technique to extract handwriting from various datasets, ending up with about 24k documents, followed by training a vision transformer on reconstructing masked patches of the handwriting. Saghog is then finetuned by appending NetRVLAD as an encoding layer to the pre- trained encoder. Evaluation of our approach on three historical datasets, Historical-WI, HisFrag20, and GRK-Papyri, demonstrates the effective- ness of Saghog for writer retrieval. Additionally, we provide ablation studies on our architecture and evaluate un- and supervised finetuning. Notably, on HisFrag20, Saghog outperforms related work with a mAP of 57.2 % - a margin of 11.6 % to the current state of the art, showcas- ing its robustness on challenging data, and is competitive on even small datasets, e.g. GRK-Papyri, where we achieve a Top-1 accuracy of 58.0 %.\n",
       "\n",
       " keywords \n",
       "----------\n",
       " Writer Retrieval ·Self-Supervised Learning ·Masked Au- toencoder ·Document Analysis.\n",
       "\n",
       " introduction \n",
       "----------\n",
       " Writer Retrieval (WR) is the task of locating documents written by the same author within a dataset, achieved by identifying similarities in handwriting [18]. This task is particularly valuable for historians and paleographers, allowing them to track individuals or social groups across various historical periods [9]. Ad- ditionally, WR is used for recognizing documents with unknown authors and uncovering similarities within such documents [6]. WR methods usually consist of multiple steps, such as sampling of handwrit- ing by applying interest point detection, a neural network for feature extraction, and feature encoding such as NetVLAD, followed by aggregation of the encoded features [29]. Although those methods work for large datasets such as Historical- WI [14], WR still lacks performance for datasets that contain less handwriting or noise such as degradation, best exemplified by HisFrag20 [32]. While approaches trained on full fragments currently work best [26], experiments show that those methods do infer features from the background, not necessarily related on the actual handwriting [30]."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
