{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panag\\Desktop\\captonomy-mle\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Paper:\n",
    "    filename: str\n",
    "    title: str = ''\n",
    "    authors: str = ''\n",
    "    abstract: str = ''\n",
    "    keywords: str = ''\n",
    "    introduction: str = ''\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f' filename \\n----------\\n {self.filename}' + \\\n",
    "               f'\\n\\n title \\n----------\\n {self.title}' + \\\n",
    "               f'\\n\\n authors \\n----------\\n {self.authors}' + \\\n",
    "               f'\\n\\n abstract \\n----------\\n {self.abstract}' + \\\n",
    "               f'\\n\\n keywords \\n----------\\n {self.keywords}' + \\\n",
    "               f'\\n\\n introduction \\n----------\\n {self.introduction}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_camel_case(text: str) -> str:\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Capitalize the first letter of each word except the first word\n",
    "    camel_case_words = [words[0].lower()] + [word.capitalize() for word in words[1:]]\n",
    "    \n",
    "    # Join the words back together without spaces\n",
    "    camel_case_text = ''.join(camel_case_words)\n",
    "    \n",
    "    return camel_case_text\n",
    "\n",
    "def authors_to_list(authors: str) -> list[str]:\n",
    "\n",
    "    split = authors.rsplit(' and ', maxsplit=1) # there should be at most one ' and '\n",
    "\n",
    "    authors_list = split[0].split(',')\n",
    "    authors_list = [author.strip() for author in authors_list]\n",
    "\n",
    "    if len(split) == 2: # there should be a last author as inteded after the ' and '\n",
    "        last_author = split[1].strip()\n",
    "        authors_list.append(last_author)\n",
    "\n",
    "    return authors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panag\\Desktop\\captonomy-mle\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved model\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "try:\n",
    "    model = SentenceTransformer(f'local-models/{model_name}')\n",
    "    print('Loaded local model')\n",
    "except:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model.save(f'local-models/{model_name}')\n",
    "    print('Downloaded and saved model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the papers and encode them into embeddings\n",
    "\n",
    "with open('papers.pkl', 'rb') as f:\n",
    "    papers: list[Paper] = pickle.load(f)\n",
    "\n",
    "papers_text = [f'Title: {paper.title} \\n Abstract: {paper.abstract}' for paper in papers]\n",
    "\n",
    "papers_emb = model.encode(papers_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categories and predict the category of each paper\n",
    "\n",
    "categories = [\n",
    "    'Tables', \n",
    "    'Classification', \n",
    "    'Key Information Extraction',\n",
    "    'Optical Character Recognition', \n",
    "    'Datasets', \n",
    "    'Document Layout Understanding', \n",
    "    'Others'\n",
    "]\n",
    "\n",
    "categories_emb = model.encode(categories)\n",
    "\n",
    "similarities = model.similarity(papers_emb, categories_emb)\n",
    "\n",
    "predictions = list(map(lambda index: categories[index], similarities.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to a CSV file for further analysis\n",
    "\n",
    "rows = []\n",
    "for paper, prediction in zip(papers, predictions):\n",
    "    rows.append({'filename': paper.filename, 'title': paper.title, 'authors': paper.authors, 'category': prediction})\n",
    "\n",
    "predictions_df = pd.DataFrame(rows)\n",
    "\n",
    "predictions_df.to_csv('similarity-preds.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions in a json file with the specified format\n",
    "\n",
    "result = {to_camel_case(category): [] for category in categories}\n",
    "\n",
    "for paper, prediction in zip(papers, predictions):\n",
    "    result[to_camel_case(prediction)].append({\"originalFileName\": paper.filename, \"title\": paper.title, \"authors\": authors_to_list(paper.authors)})\n",
    "\n",
    "with open('similarity-preds.json', 'w') as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the categories and predict the category of each paper\n",
    "\n",
    "extended_categories = [\n",
    "    'Tables are structured representations of data organized in rows and columns, often used to present numerical information, comparisons, and relationships clearly and efficiently.', \n",
    "    'Classification is the task of assigning predefined categories to text documents based on their content, enabling systematic organization and retrieval of information.', \n",
    "    'Key Information Extraction is the automatic identification and extraction of significant entities and relevant data from unstructured texts, facilitating efficient access to critical information and enhancing data organization.',\n",
    "    'Optical Character Recognition is the technology used to convert different types of documents, such as scanned paper documents and images, into editable and searchable data by recognizing and extracting printed or handwritten text.', \n",
    "    'Datasets are ollections of structured or unstructured data organized for analysis and research purposes, often used in machine learning and statistical modeling to train and evaluate algorithms.', \n",
    "    'Document Layout Understanding is the process of analyzing and interpreting the structural layout of documents to extract meaningful information about the arrangement and organization of content, including text, images, tables, and other elements.', \n",
    "    'Others are any additional tasks or methodologies related to document processing and information extraction that do not fit into the predefined categories, encompassing a variety of techniques and applications.'\n",
    "]\n",
    "\n",
    "extended_categories_emb = model.encode(extended_categories)\n",
    "\n",
    "similarities_extended = model.similarity(papers_emb, extended_categories_emb)\n",
    "\n",
    "predictions_extended = list(map(lambda index: categories[index], similarities_extended.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save extended predictions to a CSV file for further analysis\n",
    "\n",
    "rows = []\n",
    "for paper, prediction in zip(papers, predictions_extended):\n",
    "    rows.append({'filename': paper.filename, 'title': paper.title, 'authors': paper.authors, 'category': prediction})\n",
    "\n",
    "predictions_extended_df = pd.DataFrame(rows)\n",
    "\n",
    "predictions_df.to_csv('similarity-preds-ext.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save extended predictions in a json file with the specified format\n",
    "\n",
    "result = {to_camel_case(category): [] for category in categories}\n",
    "\n",
    "for paper, prediction in zip(papers, predictions_extended):\n",
    "    result[to_camel_case(prediction)].append({\"originalFileName\": paper.filename, \"title\": paper.title, \"authors\": authors_to_list(paper.authors)})\n",
    "\n",
    "with open('similarity-preds-ext.json', 'w') as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
